# -*- coding: utf-8 -*-
"""PBEB1HiENQxRnVT9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1parsrpWfbmVL6WeedrF1DZp_3PKEZagQ

* Y = target attribute (Y) with values indicating 0 (unhappy) and 1 (happy) customers
* X1 = my order was delivered on time
* X2 = contents of my order was as I expected
* X3 = I ordered everything I wanted to order
* X4 = I paid a good price for my order
* X5 = I am satisfied with my courier
* X6 = the app makes ordering easy for me

Attributes X1 to X6 indicate the responses for each question and have values from 1 to 5 where the smaller number indicates less and the higher number indicates more towards the answer.
"""

from google.colab import drive

drive.mount('/content/gdrive')

import pandas as pd

data= pd.read_csv('/content/gdrive/MyDrive/ACME-HappinessSurvey2020.csv')

data

"""We have a binary target with 2 levels. And 6 dependent variables with five level.

Considering the type of dependent and independent variables in my dataset, it makes perfect sense to build a tree-based model at first.
"""

data.info()

"""There is any missing value."""

data.Y.value_counts()

""" Decision trees often perform well on imbalanced datasets because their hierarchical structure allows them to learn signals from both classes. Also, the dataset isn't large, so I can chose decision tree as the base model. """

from sklearn.model_selection import train_test_split

y=data.Y
X=data.drop(['Y'],axis=1)
# split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1,shuffle=True,stratify=y)

from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn import metrics
from sklearn.metrics import classification_report
# Create Decision Tree classifer object
clf = DecisionTreeClassifier(random_state=1)

# Train Decision Tree Classifer
clf1 = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf1.predict(X_test)
print(classification_report(y_test, y_pred))

"""I prefer to observe which feature is better. I will drop some variables from the model respectively."""

from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from IPython.display import Image  
import pydotplus

dot_data = StringIO()
export_graphviz(clf1, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('customer.png')
Image(graph.create_png())

indep=data.Y
dep=data.drop(['Y','X2','X3','X4'],axis=1)
# split the dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(dep, indep, test_size=0.20, random_state=1,shuffle=True,stratify=y)

# Create Decision Tree classifer object
clf2 = DecisionTreeClassifier(criterion="entropy", max_depth=6,max_features='auto',random_state=1)

# Train Decision Tree Classifer
clf2 = clf2.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf2.predict(X_test)
print(classification_report(y_test, y_pred))

"""Accuracy is %85. Presicion and recall values are good enough. Presicion is a ratio of true positives to all positive preditions. This means that the model predict unhappy customers truly. %78 of prediction of happy custemers within all happy customer predictions(True and False) is predicted precisely. Recall is the ratio of true positives to sum of true predictions( True positive and False negative). Recall is also known as sensitivity.
 Precision score of 1.0 for a class 0 means that every item labelled as belonging to class 0 does indeed belong to class 0 whereas a recall of 1.0 means that every item from class 1 was labelled as belonging to class 1.
"""

clf2

"""The parameter ***entropy*** is for information gain.
The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain.
"""